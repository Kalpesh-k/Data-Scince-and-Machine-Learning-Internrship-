{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614ba296",
   "metadata": {},
   "source": [
    "# 7Problem Statement1: Build a text classification RNN ( RecurrentNeural Network) model using IMDB dataset.Import theIMDBdataset using tensorflow_datasets and perform the following tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29eba003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b523ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Kalpesh__K__\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a261f850e34775ad07ac5eec99255b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbe11d8e1f04daaaf3fe0368fc538e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Kalpesh__K__\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteWJS0HH\\imdb_review…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Kalpesh__K__\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteWJS0HH\\imdb_review…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\Kalpesh__K__\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteWJS0HH\\imdb_review…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb_reviews downloaded and prepared to C:\\Users\\Kalpesh__K__\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Next, we can load the IMDB dataset and shuffle the training data:\n",
    "\n",
    "\n",
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k', split=(tfds.Split.TRAIN, tfds.Split.TEST), with_info=True, as_supervised=True)\n",
    "train_batches = train_data.shuffle(1000).padded_batch(32, padded_shapes=([None],[]))\n",
    "test_batches = test_data.shuffle(1000).padded_batch(32, padded_shapes=([None],[]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c384136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we can encode the text data using the TextVectorization layer:\n",
    "\n",
    "\n",
    "encoder = info.features['text'].encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab059329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates an encoder that can map words to integers and back. We can use this encoder to preprocess the text data:\n",
    "\n",
    "def encode(text_tensor, label):\n",
    "  encoded_text = encoder.encode(text_tensor.numpy())\n",
    "  return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\n",
    "\n",
    "train_data = train_batches.map(encode_map_fn)\n",
    "test_data = test_batches.map(encode_map_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64af4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can build a sequential model using tf.keras.Sequential function:\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we can compile the model:\n",
    "\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e380e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can train the model using the training data:\n",
    "\n",
    "history = model.fit(train_data, epochs=10, validation_data=test_data, validation_steps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we can test the model by passing a sentence and evaluating the output:\n",
    "\n",
    "sentence = \"This movie was great!\"\n",
    "encoded_sentence = encoder.encode(sentence)\n",
    "model.predict(tf.convert_to_tensor([encoded_sentence]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58980293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca1a8a73",
   "metadata": {},
   "source": [
    "# 1Problem Statement2: Sumanth wants to build an flowers image classification CNN model which can take a flower image as input and prints the name of flower.Dataset Description:The dataset consistof 3,700 photos of flowers. The dataset contains five sub-directories, one per class:•daisy•dandelion•roses•sunflowers•tulips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c16a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38268c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = keras.utils.get_file('flower_photos', origin=data_url, untar=True)\n",
    "data_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now analyze the images present in the dataset using the following code:\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will apply cache(), shuffle(), and prefetch() functions to autotune the data and improve the data loading performance. We will also normalize the data by dividing it with 255 using the following code:\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a sequential CNN model:\n",
    "num_classes = 5\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630be2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model and printing the model summary:\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will train and validate the model using the training and validation datasets, and plot the validation results using a line plot using the following code:\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "# Plotting the validation results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe93d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will feed the system with an image of a flower and print the prediction along with the accuracy using the following code:\n",
    "\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    \"flower.jpg\", target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data by specifying the image resolution and batch size. Also, Splitting the dataset into training and validation datasets in 80:20 ratio\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "# Splitting the dataset into training and validation datasets\n",
    "data_dir = tf.keras.utils.get_file(\n",
    "    'flower_photos', 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60acdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will build a sequential CNN model using the following code:\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilethe modeland print model summary\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img_path = '/path/to/flower/image.jpg'\n",
    "img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "      .format(class_names[np.argmax(score)], 100 * np.max(score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fae50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e42704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f4e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b3749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
